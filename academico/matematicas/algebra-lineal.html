<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>√Ålgebra Lineal - Peter Shica Ramirez</title>
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;600;700&display=swap" rel="stylesheet" />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <style>
    :root {
      --primary-color: #0056b3;
      --secondary-color: #007bff;
      --text-color: #333;
      --light-gray: #f8f9fa;
      --white: #ffffff;
      --shadow: 0 4px 12px rgba(0,0,0,0.08);
      --border-radius: 10px;
      --math-color: #6f42c1;
      --accent-color: #28a745;
      --algebra-color: #e74c3c;
    }

    * { margin: 0; padding: 0; box-sizing: border-box; }

    body {
      font-family: 'Montserrat', sans-serif;
      background-color: var(--light-gray);
      color: var(--text-color);
      line-height: 1.7;
    }

    /* Header Navigation */
    .navbar {
      background-color: var(--white);
      box-shadow: var(--shadow);
      padding: 1rem 0;
      position: sticky;
      top: 0;
      z-index: 100;
    }

    .nav-container {
      max-width: 1100px;
      margin: 0 auto;
      padding: 0 1rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .logo {
      font-size: 1.5rem;
      font-weight: 700;
      color: var(--primary-color);
      text-decoration: none;
    }

    .nav-links {
      display: flex;
      gap: 2rem;
      list-style: none;
    }

    .nav-links a {
      text-decoration: none;
      color: var(--text-color);
      font-weight: 500;
      transition: color 0.3s ease;
    }

    .nav-links a:hover,
    .nav-links a.active {
      color: var(--secondary-color);
    }

    /* Breadcrumb */
    .breadcrumb {
      max-width: 900px;
      margin: 1rem auto;
      padding: 0 1rem;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      font-size: 0.9rem;
      color: #666;
    }

    .breadcrumb a {
      color: var(--secondary-color);
      text-decoration: none;
    }

    .breadcrumb a:hover {
      text-decoration: underline;
    }

    /* Article Container */
    .article-container {
      max-width: 900px;
      margin: 0 auto;
      padding: 0 1rem 3rem;
    }

    /* Article Header */
    .article-header {
      background: linear-gradient(135deg, var(--algebra-color), #ff6b6b);
      color: white;
      border-radius: var(--border-radius);
      padding: 3rem 2rem;
      margin-bottom: 2rem;
      text-align: center;
      position: relative;
      overflow: hidden;
    }

    .article-header::before {
      content: "‚äó ‚äï ‚àò ‚äô ‚äò ‚âÖ ‚àà ‚äÜ ‚à© ‚à™";
      position: absolute;
      font-size: 3rem;
      opacity: 0.15;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      letter-spacing: 1.5rem;
      white-space: nowrap;
      z-index: 0;
    }

    .header-content {
      position: relative;
      z-index: 1;
    }

    .article-title {
      font-size: 2.5rem;
      margin-bottom: 1rem;
      font-weight: 700;
    }

    .article-subtitle {
      font-size: 1.2rem;
      opacity: 0.9;
      margin-bottom: 2rem;
    }

    .article-meta {
      display: flex;
      justify-content: center;
      gap: 2rem;
      flex-wrap: wrap;
      font-size: 0.9rem;
      opacity: 0.8;
    }

    .meta-item {
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    .difficulty-badge {
      background: rgba(255,255,255,0.2);
      color: white;
      padding: 0.3rem 0.8rem;
      border-radius: 15px;
      font-weight: 500;
      border: 1px solid rgba(255,255,255,0.3);
    }

    /* Table of Contents */
    .toc {
      background: var(--white);
      border: 2px solid var(--algebra-color);
      border-radius: var(--border-radius);
      padding: 1.5rem;
      margin: 2rem 0;
      box-shadow: var(--shadow);
      max-width: 400px;
    }

    .toc h4 {
      color: var(--primary-color);
      margin-bottom: 1rem;
      font-size: 1.1rem;
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    .toc h4::before {
      content: "üìã";
      font-size: 1.2rem;
    }

    .toc ul {
      list-style: none;
      margin: 0;
    }

    .toc li {
      margin-bottom: 0.5rem;
      padding: 0.3rem 0;
      border-bottom: 1px solid #eee;
    }

    .toc li:last-child {
      border-bottom: none;
    }

    .toc a {
      text-decoration: none;
      color: var(--text-color);
      transition: color 0.3s ease;
      font-weight: 500;
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    .toc a::before {
      content: "‚ñ∏";
      color: var(--algebra-color);
      font-weight: bold;
    }

    .toc a:hover {
      color: var(--algebra-color);
    }

    /* Article Content */
    .article-content {
      background: var(--white);
      border-radius: var(--border-radius);
      box-shadow: var(--shadow);
      padding: 3rem;
      margin-bottom: 2rem;
    }

    .article-content h2 {
      color: var(--primary-color);
      font-size: 1.8rem;
      margin-top: 3rem;
      margin-bottom: 1.5rem;
      border-bottom: 2px solid var(--algebra-color);
      padding-bottom: 0.5rem;
      scroll-margin-top: 100px;
    }

    .article-content h2:first-child {
      margin-top: 0;
    }

    .article-content h3 {
      color: var(--algebra-color);
      font-size: 1.4rem;
      margin-top: 2rem;
      margin-bottom: 1rem;
      scroll-margin-top: 100px;
    }

    .article-content p {
      margin-bottom: 1.5rem;
      text-align: justify;
      line-height: 1.8;
    }

    .article-content ul,
    .article-content ol {
      margin-left: 2rem;
      margin-bottom: 1.5rem;
    }

    .article-content li {
      margin-bottom: 0.8rem;
      line-height: 1.7;
    }

    /* Math expressions */
    .math-block {
      background: #f8f9fa;
      border: 1px solid #dee2e6;
      border-radius: var(--border-radius);
      padding: 1.5rem;
      margin: 1.5rem 0;
      text-align: center;
      overflow-x: auto;
    }

    /* Info boxes */
    .info-box {
      border-radius: var(--border-radius);
      padding: 1.5rem;
      margin: 1.5rem 0;
      border-left: 4px solid;
    }

    .info-box.definition {
      background: #e3f2fd;
      border-color: #2196f3;
    }

    .info-box.theorem {
      background: #f3e5f5;
      border-color: #9c27b0;
    }

    .info-box.example {
      background: #e8f5e8;
      border-color: #4caf50;
    }

    .info-box.application {
      background: #fff3e0;
      border-color: #ff9800;
    }

    .info-box h4 {
      color: var(--primary-color);
      margin-bottom: 0.8rem;
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    /* Matrix display */
    .matrix {
      display: inline-block;
      background: #f8f9fa;
      border: 1px solid #dee2e6;
      border-radius: 5px;
      padding: 0.5rem;
      margin: 0.3rem;
    }

    /* Vector notation */
    .vector {
      font-weight: bold;
      color: var(--algebra-color);
    }

    /* Back to Top Button */
    .back-to-top {
      position: fixed;
      bottom: 2rem;
      right: 2rem;
      background: var(--algebra-color);
      color: white;
      border: none;
      border-radius: 50%;
      width: 50px;
      height: 50px;
      font-size: 1.2rem;
      cursor: pointer;
      box-shadow: var(--shadow);
      transition: all 0.3s ease;
      opacity: 0;
      visibility: hidden;
      z-index: 50;
    }

    .back-to-top.show {
      opacity: 1;
      visibility: visible;
    }

    .back-to-top:hover {
      background: var(--primary-color);
      transform: translateY(-3px);
      box-shadow: 0 6px 20px rgba(0,0,0,0.2);
    }

    /* Smooth scrolling */
    html {
      scroll-behavior: smooth;
    }

    /* Navigation between articles */
    .article-navigation {
      background: var(--white);
      border-radius: var(--border-radius);
      box-shadow: var(--shadow);
      padding: 1.5rem;
      margin: 2rem 0;
      text-align: center;
    }

    .nav-back-to-section {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      background: var(--algebra-color);
      color: white;
      text-decoration: none;
      padding: 0.8rem 1.5rem;
      border-radius: 25px;
      font-weight: 600;
      transition: all 0.3s ease;
    }

    .nav-back-to-section:hover {
      background: var(--primary-color);
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(0,0,0,0.2);
    }

    /* Responsive */
    @media (max-width: 768px) {
      .article-header {
        padding: 2rem 1rem;
      }

      .article-title {
        font-size: 2rem;
      }

      .article-content {
        padding: 2rem 1.5rem;
      }

      .article-content h2 {
        font-size: 1.5rem;
        margin-top: 2rem;
      }

      .article-content h3 {
        font-size: 1.2rem;
      }

      .toc {
        padding: 1rem;
        margin: 1rem 0;
      }

      .nav-container {
        flex-direction: column;
        gap: 1rem;
      }

      .nav-links {
        gap: 1rem;
      }

      .back-to-top {
        bottom: 1rem;
        right: 1rem;
        width: 45px;
        height: 45px;
        font-size: 1rem;
      }

      .article-navigation {
        padding: 1rem;
        margin: 1rem 0;
      }
    }
  </style>
</head>

<body>
  <!-- Navigation -->
  <nav class="navbar">
    <div class="nav-container">
      <a href="../../index.html" class="logo">Peter Shica</a>
      <ul class="nav-links">
        <li><a href="../../index.html">Inicio</a></li>
        <li><a href="../index.html" class="active">Acad√©mico</a></li>
        <li><a href="../../curriculum/index.html">Curr√≠culum</a></li>
        <li><a href="../../blog/index.html">Blog</a></li>
      </ul>
    </div>
  </nav>

  <!-- Breadcrumb -->
  <nav class="breadcrumb">
    <a href="../../index.html">Inicio</a> > 
    <a href="../index.html">Acad√©mico</a> > 
    <a href="index.html">Matem√°ticas</a> > 
    <span>√Ålgebra Lineal</span>
  </nav>

  <div class="article-container">
    <!-- Article Header -->
    <header class="article-header">
      <div class="header-content">
        <h1 class="article-title">‚äó √Ålgebra Lineal</h1>
        <p class="article-subtitle">
          Fundamentos matem√°ticos esenciales para machine learning, an√°lisis de datos y transformaciones geom√©tricas
        </p>
        <div class="article-meta">
          <div class="meta-item">
            <span>üìÖ</span>
            <span>Enero 2025</span>
          </div>
          <div class="meta-item">
            <span>‚è±Ô∏è</span>
            <span>20 min de lectura</span>
          </div>
          <div class="meta-item">
            <span class="difficulty-badge">Intermedio</span>
          </div>
        </div>
      </div>
    </header>

    <!-- Table of Contents -->
    <div class="toc">
      <h4>Contenido</h4>
      <ul>
        <li><a href="#introduccion">1. Introducci√≥n</a></li>
        <li><a href="#espacios-vectoriales">2. Espacios Vectoriales</a></li>
        <li><a href="#eigenvalores-eigenvectores">3. Eigenvalores y Eigenvectores</a></li>
        <li><a href="#descomposicion-matricial">4. Descomposici√≥n Matricial</a></li>
        <li><a href="#aplicaciones-ml">5. Aplicaciones en ML</a></li>
        <li><a href="#ejemplos-practicos">6. Ejemplos Pr√°cticos</a></li>
        <li><a href="#conclusion">7. Conclusi√≥n</a></li>
      </ul>
    </div>

    <!-- Article Content -->
    <main class="article-content">
      <h2 id="introduccion">1. Introducci√≥n</h2>
      <p>
        El <strong>√°lgebra lineal</strong> es una rama fundamental de las matem√°ticas que estudia vectores, 
        espacios vectoriales, transformaciones lineales y sistemas de ecuaciones lineales. Es la base 
        matem√°tica sobre la cual se construyen muchos algoritmos de machine learning y m√©todos de an√°lisis de datos.
      </p>

      <div class="info-box definition">
        <h4>üéØ ¬øPor qu√© es importante?</h4>
        <p>
          El √°lgebra lineal nos permite representar y manipular datos de manera eficiente, realizar 
          transformaciones geom√©tricas complejas, y entender el comportamiento de algoritmos de 
          aprendizaje autom√°tico desde una perspectiva matem√°tica rigurosa.
        </p>
      </div>

      <p>
        En este art√≠culo exploraremos los conceptos fundamentales que todo cient√≠fico de datos 
        debe dominar para comprender profundamente los algoritmos modernos de ML.
      </p>

      <h2 id="espacios-vectoriales">2. Espacios Vectoriales</h2>
      <p>
        Un <strong>espacio vectorial</strong> es una estructura algebraica que generaliza las 
        propiedades de los vectores en el plano y el espacio tridimensional.
      </p>

      <div class="info-box definition">
        <h4>üìñ Definici√≥n Formal</h4>
        <p>
          Un espacio vectorial <span class="vector">V</span> sobre un campo <strong>F</strong> 
          (usualmente ‚Ñù o ‚ÑÇ) es un conjunto equipado con dos operaciones:
        </p>
        <ul>
          <li><strong>Suma vectorial:</strong> <span class="vector">u</span> + <span class="vector">v</span> ‚àà <span class="vector">V</span></li>
          <li><strong>Multiplicaci√≥n escalar:</strong> Œ±<span class="vector">v</span> ‚àà <span class="vector">V</span> para Œ± ‚àà F</li>
        </ul>
      </div>

      <h3>2.1 Propiedades Fundamentales</h3>
      <p>
        Todo espacio vectorial debe satisfacer ocho axiomas fundamentales:
      </p>

      <ol>
        <li><strong>Conmutatividad:</strong> <span class="vector">u</span> + <span class="vector">v</span> = <span class="vector">v</span> + <span class="vector">u</span></li>
        <li><strong>Asociatividad:</strong> (<span class="vector">u</span> + <span class="vector">v</span>) + <span class="vector">w</span> = <span class="vector">u</span> + (<span class="vector">v</span> + <span class="vector">w</span>)</li>
        <li><strong>Elemento neutro:</strong> Existe <span class="vector">0</span> tal que <span class="vector">v</span> + <span class="vector">0</span> = <span class="vector">v</span></li>
        <li><strong>Elemento inverso:</strong> Para cada <span class="vector">v</span> existe -<span class="vector">v</span> tal que <span class="vector">v</span> + (-<span class="vector">v</span>) = <span class="vector">0</span></li>
      </ol>

      <h3>2.2 Combinaciones Lineales e Independencia</h3>
      <p>
        Una <strong>combinaci√≥n lineal</strong> de vectores <span class="vector">v‚ÇÅ, v‚ÇÇ, ..., v‚Çô</span> 
        es una expresi√≥n de la forma:
      </p>

      <div class="math-block">
        $$\alpha_1 \mathbf{v_1} + \alpha_2 \mathbf{v_2} + \cdots + \alpha_n \mathbf{v_n}$$
      </div>

      <div class="info-box example">
        <h4>üí° Ejemplo: ‚Ñù¬≥</h4>
        <p>
          En el espacio tridimensional, los vectores can√≥nicos son:
        </p>
        <div class="math-block">
          $$\mathbf{e_1} = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \quad 
            \mathbf{e_2} = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \quad 
            \mathbf{e_3} = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$$
        </div>
        <p>
          Cualquier vector en ‚Ñù¬≥ puede expresarse como combinaci√≥n lineal de estos vectores base.
        </p>
      </div>

      <h2 id="eigenvalores-eigenvectores">3. Eigenvalores y Eigenvectores</h2>
      <p>
        Los <strong>eigenvalores</strong> y <strong>eigenvectores</strong> son conceptos centrales 
        que revelan las direcciones principales de transformaci√≥n de una matriz.
      </p>

      <div class="info-box definition">
        <h4>üìñ Definici√≥n</h4>
        <p>
          Para una matriz <strong>A</strong> de n√ón, un vector no nulo <span class="vector">v</span> 
          es un <strong>eigenvector</strong> con <strong>eigenvalor</strong> Œª si:
        </p>
        <div class="math-block">
          $$A\mathbf{v} = \lambda\mathbf{v}$$
        </div>
      </div>

      <h3>3.1 C√°lculo de Eigenvalores</h3>
      <p>
        Los eigenvalores se obtienen resolviendo la ecuaci√≥n caracter√≠stica:
      </p>

      <div class="math-block">
        $$\det(A - \lambda I) = 0$$
      </div>

      <p>
        Donde <strong>I</strong> es la matriz identidad de la misma dimensi√≥n que <strong>A</strong>.
      </p>

      <div class="info-box example">
        <h4>üí° Ejemplo Pr√°ctico</h4>
        <p>
          Consideremos la matriz:
        </p>
        <div class="math-block">
          $$A = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix}$$
        </div>
        <p>
          La ecuaci√≥n caracter√≠stica es:
        </p>
        <div class="math-block">
          $$\det\begin{pmatrix} 3-\lambda & 1 \\ 1 & 3-\lambda \end{pmatrix} = (3-\lambda)^2 - 1 = 0$$
        </div>
        <p>
          Resolviendo: Œª‚ÇÅ = 4, Œª‚ÇÇ = 2
        </p>
      </div>

      <h3>3.2 Interpretaci√≥n Geom√©trica</h3>
      <p>
        Los eigenvectores representan las <strong>direcciones principales</strong> de la transformaci√≥n, 
        mientras que los eigenvalores indican <strong>cu√°nto se estira o comprime</strong> el espacio 
        en esas direcciones.
      </p>

      <h2 id="descomposicion-matricial">4. Descomposici√≥n Matricial</h2>
      <p>
        La descomposici√≥n matricial es una t√©cnica fundamental que expresa una matriz como 
        producto de matrices m√°s simples, revelando estructura interna y simplificando c√°lculos.
      </p>

      <h3>4.1 Descomposici√≥n Espectral</h3>
      <p>
        Para matrices sim√©tricas, la <strong>descomposici√≥n espectral</strong> es particularmente √∫til:
      </p>

      <div class="math-block">
        $$A = Q\Lambda Q^T$$
      </div>

      <p>
        Donde <strong>Q</strong> es una matriz ortogonal cuyos columnas son los eigenvectores 
        normalizados, y <strong>Œõ</strong> es una matriz diagonal con los eigenvalores.
      </p>

      <h3>4.2 Descomposici√≥n en Valores Singulares (SVD)</h3>
      <p>
        La <strong>SVD</strong> es una generalizaci√≥n poderosa que funciona para cualquier matriz rectangular:
      </p>

      <div class="math-block">
        $$A = U\Sigma V^T$$
      </div>

      <div class="info-box theorem">
        <h4>üéì Teorema SVD</h4>
        <p>
          Toda matriz <strong>A</strong> de m√ón puede descomponerse como <strong>A = UŒ£V^T</strong> donde:
        </p>
        <ul>
          <li><strong>U</strong>: matriz ortogonal m√óm</li>
          <li><strong>Œ£</strong>: matriz diagonal m√ón con valores singulares</li>
          <li><strong>V</strong>: matriz ortogonal n√ón</li>
        </ul>
      </div>

      <h3>4.3 Descomposici√≥n LU</h3>
      <p>
        La <strong>descomposici√≥n LU</strong> factoriza una matriz como producto de una 
        matriz triangular inferior (<strong>L</strong>) y una superior (<strong>U</strong>):
      </p>

      <div class="math-block">
        $$A = LU$$
      </div>

      <p>
        Esta descomposici√≥n es especialmente √∫til para resolver sistemas de ecuaciones lineales 
        de manera eficiente.
      </p>

      <h2 id="aplicaciones-ml">5. Aplicaciones en Machine Learning</h2>
      <p>
        El √°lgebra lineal es fundamental en pr√°cticamente todos los algoritmos de machine learning. 
        Aqu√≠ exploramos las aplicaciones m√°s importantes.
      </p>

      <h3>5.1 Principal Component Analysis (PCA)</h3>
      <p>
        <strong>PCA</strong> utiliza eigenvalores y eigenvectores para reducir la dimensionalidad 
        de los datos manteniendo la m√°xima varianza:
      </p>

      <div class="info-box application">
        <h4>üîß Algoritmo PCA</h4>
        <ol>
          <li>Centrar los datos: XÃÉ = X - Œº</li>
          <li>Calcular la matriz de covarianza: C = (1/n)XÃÉXÃÉ^T</li>
          <li>Encontrar eigenvalores y eigenvectores de C</li>
          <li>Ordenar por eigenvalor decreciente</li>
          <li>Seleccionar los k primeros eigenvectores</li>
        </ol>
      </div>

      <h3>5.2 Regresi√≥n Lineal</h3>
      <p>
        La regresi√≥n lineal se resuelve usando √°lgebra lineal mediante la <strong>ecuaci√≥n normal</strong>:
      </p>

      <div class="math-block">
        $$\boldsymbol{\theta} = (X^TX)^{-1}X^T\mathbf{y}$$
      </div>

      <p>
        Donde <strong>X</strong> es la matriz de caracter√≠sticas y <span class="vector">y</span> 
        es el vector de valores objetivo.
      </p>

      <h3>5.3 Redes Neuronales</h3>
      <p>
        En redes neuronales, cada capa realiza una <strong>transformaci√≥n lineal</strong> 
        seguida de una funci√≥n de activaci√≥n no lineal:
      </p>

      <div class="math-block">
        $$\mathbf{h} = f(W\mathbf{x} + \mathbf{b})$$
      </div>

      <p>
        Donde <strong>W</strong> es la matriz de pesos, <span class="vector">x</span> es la entrada, 
        <span class="vector">b</span> es el vector de sesgos, y <strong>f</strong> es la funci√≥n de activaci√≥n.
      </p>

      <h3>5.4 Transformaciones Geom√©tricas</h3>
      <p>
        Las transformaciones como rotaci√≥n, escalado y traslaci√≥n se representan mediante matrices:
      </p>

      <div class="info-box example">
        <h4>üí° Matriz de Rotaci√≥n 2D</h4>
        <div class="math-block">
          $$R(\theta) = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$$
        </div>
        <p>
          Esta matriz rota un vector en el plano por un √°ngulo Œ∏ en sentido antihorario.
        </p>
      </div>

      <h2 id="ejemplos-practicos">6. Ejemplos Pr√°cticos</h2>

      <h3>6.1 Compresi√≥n de Im√°genes con SVD</h3>
      <p>
        La SVD permite comprimir im√°genes manteniendo la informaci√≥n m√°s importante:
      </p>

      <div class="info-box application">
        <h4>üñºÔ∏è Algoritmo de Compresi√≥n</h4>
        <ol>
          <li>Representar la imagen como matriz A</li>
          <li>Calcular SVD: A = UŒ£V^T</li>
          <li>Mantener solo los k valores singulares m√°s grandes</li>
          <li>Reconstruir: A_k = U_k Œ£_k V_k^T</li>
        </ol>
        <p>
          La imagen reconstruida tendr√° menor tama√±o pero mantendr√° las caracter√≠sticas principales.
        </p>
      </div>

      <h3>6.2 Sistema de Recomendaci√≥n</h3>
      <p>
        Los sistemas de recomendaci√≥n utilizan factorizaci√≥n matricial para predecir preferencias:
      </p>

      <div class="math-block">
        $$R \approx UV^T$$
      </div>

      <p>
        Donde <strong>R</strong> es la matriz usuario-√≠tem, <strong>U</strong> representa 
        caracter√≠sticas de usuarios y <strong>V</strong> caracter√≠sticas de √≠tems.
      </p>

      <h3>6.3 An√°lisis de Componentes Independientes (ICA)</h3>
      <p>
        ICA separa se√±ales mezcladas encontrando componentes estad√≠sticamente independientes, 
        √∫til en procesamiento de se√±ales y neurociencia.
      </p>

      <h2 id="conclusion">7. Conclusi√≥n</h2>
      <p>
        El √°lgebra lineal proporciona las herramientas matem√°ticas fundamentales para:
      </p>

      <ul>
        <li><strong>Representaci√≥n eficiente</strong> de datos multidimensionales</li>
        <li><strong>Transformaciones geom√©tricas</strong> y manipulaci√≥n del espacio</li>
        <li><strong>Reducci√≥n de dimensionalidad</strong> preservando informaci√≥n relevante</li>
        <li><strong>Optimizaci√≥n</strong> de algoritmos de machine learning</li>
        <li><strong>An√°lisis de estabilidad</strong> en sistemas din√°micos</li>
      </ul>

      <div class="info-box application">
        <h4>‚úÖ Pr√≥ximos pasos</h4>
        <p>
          Para profundizar en √°lgebra lineal aplicada, considera explorar:
        </p>
        <ul>
          <li>M√©todos num√©ricos para factorizaci√≥n matricial</li>
          <li>√Ålgebra lineal esparsa para big data</li>
          <li>Tensores y √°lgebra multilineal</li>
          <li>Aplicaciones en deep learning y computer vision</li>
        </ul>
      </div>

      <div class="info-box example">
        <h4>üîó Recursos Recomendados</h4>
        <ul>
          <li><strong>Libros:</strong> "Linear Algebra and Its Applications" - Gilbert Strang</li>
          <li><strong>Herramientas:</strong> NumPy, SciPy, MATLAB para implementaci√≥n pr√°ctica</li>
          <li><strong>Visualizaci√≥n:</strong> 3Blue1Brown - Essence of Linear Algebra</li>
        </ul>
      </div>
    </main>

    <!-- Article Navigation -->
    <nav class="article-navigation">
      <a href="index.html" class="nav-back-to-section">
        ‚Üê Volver a Matem√°ticas
      </a>
    </nav>
  </div>

  <!-- Back to Top Button -->
  <button class="back-to-top" id="backToTop" title="Volver arriba">
    ‚Üë
  </button>

  <footer style="text-align: center; padding: 2rem; color: #777; background: white; margin-top: 3rem; border-top: 1px solid #eee;">
    ¬© 2025 Peter Shica ‚Äî Todos los derechos reservados
  </footer>

  <script>
    // Back to top functionality
    window.addEventListener('scroll', function() {
      const backToTopButton = document.getElementById('backToTop');
      if (window.pageYOffset > 300) {
        backToTopButton.classList.add('show');
      } else {
        backToTopButton.classList.remove('show');
      }
    });

    document.getElementById('backToTop').addEventListener('click', function() {
      window.scrollTo({
        top: 0,
        behavior: 'smooth'
      });
    });

    // Add smooth scrolling to TOC links
    document.addEventListener('DOMContentLoaded', function() {
      const tocLinks = document.querySelectorAll('.toc a');
      tocLinks.forEach(link => {
        link.addEventListener('click', function(e) {
          e.preventDefault();
          const targetId = this.getAttribute('href');
          const targetElement = document.querySelector(targetId);
          if (targetElement) {
            const offsetTop = targetElement.offsetTop - 100;
            window.scrollTo({
              top: offsetTop,
              behavior: 'smooth'
            });
          }
        });
      });
    });
  </script>

</body>
</html>
